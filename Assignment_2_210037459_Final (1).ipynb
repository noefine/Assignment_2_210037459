{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-GBUfVzlRVWv",
        "FphILzg_UFuJ",
        "o2YDpBNIXJs_",
        "Wi8u5A6EY6UE",
        "13tt3_nvaCtY",
        "epbvpwfheAkN",
        "WdxO9M_0iTui",
        "G1q1EqoCk3Ms",
        "jEaC6DPZoXY6",
        "5mQJGi2wqx7w",
        "eO6pqML-u6tc",
        "BO6QdbYbvpcm",
        "tKOj6py9v5ht",
        "Uh0JfU2KyxbD",
        "EcckD8FuzvO0",
        "SvVNaOpg0tCj",
        "lty9B7YK2Hzg",
        "zQ4egCLP2hNB",
        "aFF_idMY4I4x",
        "Gv0pKvvS5GDJ",
        "JT8-xO5u5skU",
        "oGv-sdz66piW",
        "lIri-ECV7qS-",
        "LtcvdFiN8O7t",
        "7gr6EjJw_JBN",
        "p2FlmrJm_50k",
        "Y5q9PJOiBbdm",
        "p1GFCOOzCWPh",
        "n8TKQQpMD3g5",
        "1rg_5BjvE6Id",
        "8zyLwnEKIpG_",
        "LqIUE9qjJYya",
        "M1wlG5QtMUsQ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Assignment 2 – GG3209 – Python Part\n",
        "\n",
        "## Introduction\n",
        "**Student ID:** 210037459\n",
        "\n",
        "**GitHub Repository:** https://github.com/noefine/Assignment_2_210037459"
      ],
      "metadata": {
        "id": "Wfm6f0_gQB6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Python Basics"
      ],
      "metadata": {
        "id": "-GBUfVzlRVWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Create a script that calculates the average of a list of numbers:\n",
        "\n",
        "* In the following cell, I created a list of numbers [1, 2, 3, 4, 5]\n",
        "* Then, I created a function that returned the average of a list of numbers\n",
        "* I called the function with my list of numbers\n",
        "* I printed my result"
      ],
      "metadata": {
        "id": "FphILzg_UFuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0ug-wY3Pm5V"
      },
      "outputs": [],
      "source": [
        "numbers = [1,2,3,4,5]\n",
        "def calculate_average (numlist) :\n",
        "    average = sum(numlist)/len(numlist)\n",
        "    return average\n",
        "result = calculate_average(numbers)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 What's the purpose\n",
        "\n",
        "* In the following cell, I describe the purpose of the first two expressions of the code below and explain what they do\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "dat = pd.read_csv('data/world_cities.csv')  ## Import CSV file\n",
        "geom = gpd.points_from_xy(dat['lon'], dat['lat'])\n",
        "geom = gpd.GeoSeries(geom)\n",
        "dat = gpd.GeoDataFrame(dat, geometry = gpd.GeoSeries(geom), crs = 4326)\n",
        "dat.to_file('output/world_cities.shp')      ## Export Shapefile\n",
        "```"
      ],
      "metadata": {
        "id": "o2YDpBNIXJs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first expression, **import pandas as pd**, imports the Pandas library and gives it the shortcut name pd. Pandas is used for working with data tables. The second expression, **import geopandas as gpd**, imports GeoPandas, a library that extends Pandas for working with spatial/geographic data. It gives GeoPandas the shortcut name gpd."
      ],
      "metadata": {
        "id": "5z-Yc1JMXxDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Indentation\n",
        "\n",
        "* In the following cell, I address the indentation issue in the following code by deleting the indentation on the second line."
      ],
      "metadata": {
        "id": "Wi8u5A6EY6UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixed code:\n",
        "name = 'Dave'\n",
        "dogs = 0\n",
        "print('My name is', name, 'and I own', dogs, 'dogs.')"
      ],
      "metadata": {
        "id": "FV6OKWGaZsO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Strings\n",
        "* In the following code cell, I create a script where I print a string and integer variables"
      ],
      "metadata": {
        "id": "13tt3_nvaCtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"I am \"\n",
        "b = 20\n",
        "c = \" years old.\"\n",
        "print(a,b,c)"
      ],
      "metadata": {
        "id": "IPqfzIWEaWpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 6 Nested *if* conditions\n",
        "\n"
      ],
      "metadata": {
        "id": "epbvpwfheAkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the following cell, I will fix the indentation issues in the code"
      ],
      "metadata": {
        "id": "s9BHOWqVuuEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixed code:\n",
        "season = \"Winter\"\n",
        "temperature = 10\n",
        "\n",
        "if season == \"Winter\":\n",
        "\n",
        "     if temperature > 7:\n",
        "         print(\"No need for winter jacket!\")\n",
        "\n",
        "     else:\n",
        "         print(\"It might be cold! Wear a proper jacket!\")\n",
        "\n",
        "elif season == \"Summer\":\n",
        "\n",
        "     if temperature > 20:\n",
        "         print(\"It's warm! Time to wear shorts!\")\n",
        "\n",
        "     else:\n",
        "         print(\"Well this is Finland, better wear long trousers!\")\n",
        "else:\n",
        "     print(\"Check the weather forecast!\")"
      ],
      "metadata": {
        "id": "ZQO25dZne5wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the following code cell, I create a script with several nested *if* conditions."
      ],
      "metadata": {
        "id": "BvcZGU0-fQ4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The price of movie tickets for an individual at the movie theatre:\n",
        "# Define the customer's age and membership status\n",
        "age = 16\n",
        "has_membership = True\n",
        "\n",
        "# Check age category first\n",
        "if age < 12:\n",
        "    # Child tickets\n",
        "    if has_membership:\n",
        "        print(\"Child ticket with membership discount: $6\")\n",
        "    else:\n",
        "        print(\"Child ticket: $8\")\n",
        "\n",
        "elif age < 18:\n",
        "    # Teen tickets\n",
        "    if has_membership:\n",
        "        print(\"Teen ticket with membership discount: $9\")\n",
        "    else:\n",
        "        print(\"Teen ticket: $12\")\n",
        "\n",
        "elif age < 65:\n",
        "    # Adult tickets\n",
        "    if has_membership:\n",
        "        print(\"Adult ticket with membership discount: $13\")\n",
        "    else:\n",
        "        print(\"Adult ticket: $16\")\n",
        "\n",
        "else:\n",
        "    # Senior tickets\n",
        "    if has_membership:\n",
        "        print(\"Senior ticket with membership discount: $7\")\n",
        "    else:\n",
        "        print(\"Senior ticket: $9\")"
      ],
      "metadata": {
        "id": "dOX0AApZfcM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 Functions\n",
        "\n",
        "* In the following code cell, I create a function that transforms miles to kms\n",
        "* Then, I use the print function to show the results"
      ],
      "metadata": {
        "id": "WdxO9M_0iTui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to convert miles to kilometers\n",
        "def miles_to_km(miles):\n",
        "    #Converts miles to kilometers (1 mile = 1.60934 kilometers)\n",
        "\n",
        "    km = miles * 1.60934\n",
        "    return km\n",
        "\n",
        "#Define the miles distance\n",
        "miles_distance = 10\n",
        "#Call the function using the miles distance\n",
        "km_distance = miles_to_km(miles_distance)\n",
        "#print result\n",
        "print(km_distance)"
      ],
      "metadata": {
        "id": "x01LECupitL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8 Create a script that generates a multiplication table:\n",
        "\n",
        "* In the following code cell, I write a function that takes an integer as input and generates a multiplication table for that number, from 1 to 10.\n",
        "* I, then, call the function with a few different numbers (3, 7, 23) and print the results."
      ],
      "metadata": {
        "id": "G1q1EqoCk3Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate multiplication table as a list\n",
        "def multiplication_table(integer):\n",
        "    # create an empty list\n",
        "    results = []\n",
        "    #calculate multiplication\n",
        "    for i in range(1, 11):\n",
        "        results.append(integer * i)\n",
        "    #return the results list\n",
        "    return results\n",
        "\n",
        "# Call the function for a few numbers\n",
        "table_3 = multiplication_table(3)\n",
        "table_7 = multiplication_table(7)\n",
        "table_23 = multiplication_table(23)\n",
        "\n",
        "# Print the results outside the function\n",
        "print(\"Multiplication table for 3:\", table_3)\n",
        "print(\"Multiplication table for 7:\", table_7)\n",
        "print(\"Multiplication table for 23:\", table_23)"
      ],
      "metadata": {
        "id": "77oM2o3hlJuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.9 Loops\n",
        "\n",
        "* In the following code cell, I define a list named *lunch*\n",
        "* Then, I use a **for loop** and **conditionals** to create a dictionary with the count of each item in lunch.\n",
        "* Lastly, I print the result\n"
      ],
      "metadata": {
        "id": "jEaC6DPZoXY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list called lunch\n",
        "lunch = [\n",
        "    'Salad',\n",
        "    'Salad',\n",
        "    'Apple',\n",
        "    'Orange',\n",
        "    'Orange',\n",
        "    'Egg',\n",
        "    'Beef',\n",
        "    'Potato',\n",
        "    'Tea',\n",
        "    'Soup',\n",
        "    'Potato',\n",
        "    'Potato',\n",
        "    'Potato',\n",
        "    'Coffee'\n",
        "]\n",
        "\n",
        "# Create an empty dictionary to store the count of each item\n",
        "lunch_count = {}\n",
        "# Loop through each item in the lunch list\n",
        "for i in lunch:\n",
        "  # Check if the item is already in the dictionary\n",
        "    if i in lunch_count:\n",
        "        #if it is in the dictionary, increment its count by 1\n",
        "        lunch_count[i] += 1\n",
        "    else:\n",
        "         # if not, add it to the dictionary with count 1\n",
        "        lunch_count[i] = 1\n",
        "#Print the final dictionary with the count for each item\n",
        "print(lunch_count)"
      ],
      "metadata": {
        "id": "dBG-X76ipBRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.10 Read files\n",
        "\n",
        "* In the following code cell, I create a script that reads and prints the **latest_earthquake_world.csv** file\n",
        "* I use **pandas** to print the first 30 rows."
      ],
      "metadata": {
        "id": "5mQJGi2wqx7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file from my google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "earthquakes = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Latest_earthquake_world.csv')\n",
        "\n",
        "# Print the first 30 rows\n",
        "earthquakes.head(30)"
      ],
      "metadata": {
        "id": "ITKm997UrAB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Numpy and Pandas"
      ],
      "metadata": {
        "id": "eO6pqML-u6tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practicing NumPy"
      ],
      "metadata": {
        "id": "NxashHnXvj_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.0 Import numpy under the alias np"
      ],
      "metadata": {
        "id": "BO6QdbYbvpcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the numpy library with shortcut np\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Y8OEUhwmv01v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Create the following arrays\n",
        "\n",
        "* Create an array of 10 ones.\n",
        "\n",
        "* Create an array of the integers 1 to 20.\n",
        "\n",
        "* Create a 5 x 5 matrix of ones with a dtype int."
      ],
      "metadata": {
        "id": "tKOj6py9v5ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 1D array of length 10, filled with 1s using np.tile(value, count)\n",
        "arr1 = np.tile(1, 10)\n",
        "\n",
        "# Create a 1D array with values from 1 to 20 using np.arange(start, stop, step)\n",
        "arr2 = np.arange(1, 21, 1, dtype=\"int\")\n",
        "\n",
        "# Create a 3D array of ones with np.ones(shape) with the shape of (1, 5, 5) filled with ones\n",
        "arr3 = np.ones((1, 5, 5), dtype=\"int\")\n",
        "\n",
        "# Print the arrays\n",
        "print(arr1)\n",
        "print(arr2)\n",
        "print(arr3)"
      ],
      "metadata": {
        "id": "ycV76AmIwIq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Use numpy to:\n",
        "* Create an 3D matrix of 3 x 3 x 3 full of random numbers drawn from a standard normal distribution\n",
        "* Reshape the above array into shape (27,)"
      ],
      "metadata": {
        "id": "Uh0JfU2KyxbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 3x3x3 array with random numbers from a standard normal distribution using np.random.randn(dim1, dim2, dim3)\n",
        "arr4 = np.random.randn(3, 3, 3)\n",
        "\n",
        "# Reshape the 3D array into a 1D array of length 27\n",
        "reshapedarr4 = arr4.reshape(27,)\n",
        "\n",
        "# Print the original 3D array\n",
        "print(arr4)\n",
        "\n",
        "# Print the reshaped array\n",
        "print(reshapedarr4)"
      ],
      "metadata": {
        "id": "r5d7kf98zBVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Create an array of 20 linearly spaced numbers between 1 and 10"
      ],
      "metadata": {
        "id": "EcckD8FuzvO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an array of 20 linearly spaced numbers between 1 and 10 using np.linspace(start, stop, count) with count being the number of evenly spaced numbers\n",
        "arr5 = np.linspace(1,10,20)\n",
        "\n",
        "#Print the array\n",
        "print(arr5)"
      ],
      "metadata": {
        "id": "ZcqtmzFAz24b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 Run the following code to create an array of shape 4 x 4 and then use indexing to produce the outputs shown below."
      ],
      "metadata": {
        "id": "SvVNaOpg0tCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.arange(1, 26).reshape(5, -1)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "VaMEVWX004Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to get the output of 20:\n",
        "print(a[3,4])"
      ],
      "metadata": {
        "id": "lk7MRBUV07oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to get the output of:\n",
        "#  [[ 9 10]\n",
        "#  [14 15]\n",
        "#  [19 20]\n",
        "#  [24 25]]\n",
        "\n",
        "print(a[1:, 3:])"
      ],
      "metadata": {
        "id": "mQYD4A2l1DCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to get the output of [ 6  7  8  9 10]:\n",
        "print(a[1,:])"
      ],
      "metadata": {
        "id": "Eb-W0x7i11C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.5 Calculate the sum of all numbers in a"
      ],
      "metadata": {
        "id": "lty9B7YK2Hzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the sum of all numbers in a using np.sum(array)\n",
        "asum = np.sum(a)\n",
        "\n",
        "#Print the result\n",
        "print(asum)"
      ],
      "metadata": {
        "id": "1LtJJfSz2MLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6 Calculate the sum of each row in a"
      ],
      "metadata": {
        "id": "zQ4egCLP2hNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the sum of each row using np.sum(array, axis=1)\n",
        "arowsums = np.sum(a, axis=1)\n",
        "\n",
        "#Print the sums of each row\n",
        "print(arowsums)"
      ],
      "metadata": {
        "id": "frFFFY162lwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.7 Extract all values of a greater than the mean of a"
      ],
      "metadata": {
        "id": "aFF_idMY4I4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean of all values in a using np.mean(array)\n",
        "amean = np.mean(a)\n",
        "\n",
        "# Select values that are greater than the mean\n",
        "greaterthanmean = a[a > amean]\n",
        "\n",
        "# Print the valus greater than the mean\n",
        "print(greaterthanmean)"
      ],
      "metadata": {
        "id": "_E8FGmma4O4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practicing Pandas"
      ],
      "metadata": {
        "id": "aOU2C4635C8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.8 Import pandas under alias pd"
      ],
      "metadata": {
        "id": "Gv0pKvvS5GDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Import the pandas library with shortcut pd\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pTYSfO455YED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.9 Import the carbon footprint dataset as a dataframe named 'df'"
      ],
      "metadata": {
        "id": "JT8-xO5u5skU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 'df' as the CSV file from the URL <https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-18/food_consumption.csv>\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-18/food_consumption.csv')\n",
        "\n",
        "# Display the first few rows to make sure it loaded correctly\n",
        "df.head()"
      ],
      "metadata": {
        "id": "In2qh6eF53Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.10 How many rows and columns are there in the data frame?"
      ],
      "metadata": {
        "id": "oGv-sdz66piW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shape of the DataFrame and unpack into two variables (rows, columns)\n",
        "rows, columns = df.shape\n",
        "\n",
        "#Print the results\n",
        "print(\"Number of rows:\", rows)\n",
        "print(\"Number of columns:\", columns)"
      ],
      "metadata": {
        "id": "NKvNUZJ76v6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.11 What is the mean `co2_emission` of the whole dataset?"
      ],
      "metadata": {
        "id": "lIri-ECV7qS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean of the 'co2_emmission' column in df using .mean()\n",
        "co2_emissions_mn = df[\"co2_emmission\"].mean()\n",
        "\n",
        "#Print the mean\n",
        "print(co2_emissions_mn)"
      ],
      "metadata": {
        "id": "-ulvl4kB712e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.12 What is the maximum `co2_emmission` in the dataset and which food type and country does it belong to?"
      ],
      "metadata": {
        "id": "LtcvdFiN8O7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the maximum value of the 'co2_emmission' column in df uisng .max()\n",
        "co2_emissions_max = df[\"co2_emmission\"].max()\n",
        "\n",
        "#  Select the row where the CO2 emission equals the maximum value\n",
        "max_co2_row = df[df[\"co2_emmission\"] == co2_emissions_max]\n",
        "\n",
        "# Get the food category from the row with maximum CO2 emission\n",
        "food_type = max_co2_row[\"food_category\"].values[0]\n",
        "# Get the country from the row with maximum CO2 emission\n",
        "country = max_co2_row[\"country\"].values[0]\n",
        "\n",
        "# Print the results\n",
        "print(\"Maximum CO2 emission: \", co2_emissions_max)\n",
        "print(\"Food type: \", food_type)\n",
        "print(\"Country: \", country)"
      ],
      "metadata": {
        "id": "CzMTn4NZ8YZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.13 How many countries produce more than 1000 Kg CO2/person/year for at least one food type?"
      ],
      "metadata": {
        "id": "7gr6EjJw_JBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select rows where CO2 emission is greater than 1000\n",
        "high_emission_df = df[df[\"co2_emmission\"] > 1000]\n",
        "\n",
        "# Count the number of unique countries in the filtered DataFrame (high_emission_df) using .nunique()\n",
        "countries_with_high_emission = high_emission_df[\"country\"].nunique()\n",
        "\n",
        "#Print the results\n",
        "print(countries_with_high_emission)"
      ],
      "metadata": {
        "id": "bnYjbYye_OCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.14 Which country consumes the least amount of beef per person per year?"
      ],
      "metadata": {
        "id": "p2FlmrJm_50k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter df to include only rows where the food category is \"Beef\"\n",
        "beef_consumption_df = df[df[\"food_category\"] == \"Beef\"]\n",
        "\n",
        "# Find the row with the minimum CO2 emission for beef consumption using .idxmin() to get the index of the row and .loc to select the row as a series\n",
        "least_beef_country_row = beef_consumption_df.loc[beef_consumption_df[\"co2_emmission\"].idxmin()]\n",
        "\n",
        "# Extract the country name from that row\n",
        "least_beef_country = least_beef_country_row[\"country\"]\n",
        "\n",
        "# Extract the CO2 emission value from that row\n",
        "least_beef_consumption = least_beef_country_row[\"co2_emmission\"]\n",
        "\n",
        "# Print the results\n",
        "print(\"The country that consumes the least amount of beef per person per year is \", least_beef_country, \" with \", least_beef_consumption, \" kg CO2/person/year.\")"
      ],
      "metadata": {
        "id": "_8vYoWlx_-2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.15 What is the total emissions of all the meat products (Pork, Poultry, Fish, Lamb & Goat, Beef) in the dataset combined?"
      ],
      "metadata": {
        "id": "Y5q9PJOiBbdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of meat products\n",
        "meat_products = [\"Pork\", \"Poultry\", \"Fish\", \"Lamb & Goat\", \"Beef\"]\n",
        "\n",
        "# Filter df to include only rows where the food category is in the meat_products list using .isin(list)\n",
        "meat_df = df[df[\"food_category\"].isin(meat_products)]\n",
        "\n",
        "# Calculate the total CO2 emissions for the meat products using .sum() on the filtered DataFrame\n",
        "total_meat_emissions = meat_df[\"co2_emmission\"].sum()\n",
        "\n",
        "# Print the total CO2 emissions from meat consumption\n",
        "print(total_meat_emissions)"
      ],
      "metadata": {
        "id": "tbxghRZbBh44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.16 What is the total emissions of all other (non-meat) products in the dataset combined?"
      ],
      "metadata": {
        "id": "p1GFCOOzCWPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter df to include only non-meat food categories by using ~ to invert the boolean mask\n",
        "non_meat_df = df[~df[\"food_category\"].isin(meat_products)]\n",
        "\n",
        "# Calculate the total CO2 emissions for non-meat food products using .sum()\n",
        "total_non_meat_emissions = non_meat_df[\"co2_emmission\"].sum()\n",
        "\n",
        "#Print the result\n",
        "print(total_non_meat_emissions)"
      ],
      "metadata": {
        "id": "GcJXbgcxCcYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Exercise"
      ],
      "metadata": {
        "id": "C5fcyhouDH6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read the `world_cities.csv` file as a Pandas DataFrame"
      ],
      "metadata": {
        "id": "n8TKQQpMD3g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define 'world_cities' as the CSV file world_cities.csv\n",
        "world_cities = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/world_cities.csv')\n",
        "\n",
        "# Display the first few rows to make sure it loaded correctly\n",
        "world_cities.head()"
      ],
      "metadata": {
        "id": "sGvqAjXtDpfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate a new column named \"pop_M\" (population in millions) by transforming the \"pop\" (population) column."
      ],
      "metadata": {
        "id": "1rg_5BjvE6Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the new column 'pop_M' by dividing the values in 'pop' by one million\n",
        "world_cities[\"pop_M\"] = world_cities[\"pop\"] / 1000000"
      ],
      "metadata": {
        "id": "08kcwMAqFGwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the original 'pop' column using inplace=True to modify the DataFrame directly\n",
        "world_cities.drop(columns=\"pop\", inplace=True)"
      ],
      "metadata": {
        "id": "SXf1d_mAL-ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows to verify the changes\n",
        "world_cities.head()"
      ],
      "metadata": {
        "id": "mXzZiqBSJHoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Choose/subset a city that starts with the same letter as your first name (N)"
      ],
      "metadata": {
        "id": "8zyLwnEKIpG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset the DataFrame to only cities that start with \"N\"\n",
        "cities_starting_N = world_cities[world_cities[\"city\"].str.startswith(\"N\")]\n",
        "\n",
        "# Display the cities\n",
        "cities_starting_N"
      ],
      "metadata": {
        "id": "fk-AW0qCr_pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After looking at cities_starting_N, I choose Nuba in Palestine."
      ],
      "metadata": {
        "id": "5Wh3yVniIt2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Subset the five biggest cities from the country where your selected city is (Palestine)"
      ],
      "metadata": {
        "id": "LqIUE9qjJYya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame for cities in Palestine\n",
        "palestine_cities = world_cities[world_cities[\"country\"] == \"Palestine\"]\n"
      ],
      "metadata": {
        "id": "172iGF0YIReO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the cities by population in descending order by using .sort_values to sort by value and ascending=False to make the largest population come first\n",
        "palestine_cities_sorted = palestine_cities.sort_values(by=\"pop_M\", ascending=False)\n"
      ],
      "metadata": {
        "id": "h9UkabTQLclI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the top 5 cities in the new sorted DataFrame\n",
        "top5_palestine = palestine_cities_sorted.head(5)\n",
        "\n",
        "# Print the results\n",
        "top5_palestine"
      ],
      "metadata": {
        "id": "oCU-TJRVLfdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: GeoPandas and Raster Rio"
      ],
      "metadata": {
        "id": "M1wlG5QtMUsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Load the required data in your Drive"
      ],
      "metadata": {
        "id": "6KMcgqNOQuz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive to access data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1cL8LgfRwcGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install python packages\n",
        "!pip install contextily"
      ],
      "metadata": {
        "id": "CDyEb4G4i0wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mapclassify"
      ],
      "metadata": {
        "id": "UiFwOAUWi-xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install earthpy rasterio"
      ],
      "metadata": {
        "id": "28gN0W6yjAA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import geopandas as gpd\n",
        "import contextily as ctx\n",
        "import rasterio as rio\n",
        "from rasterio import plot"
      ],
      "metadata": {
        "id": "a1E6LwQJjBnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af5c7966"
      },
      "source": [
        "### 3.2 Practicing GeoPandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.1 Read the Census20_LSOA.shp layer into a GeoDataFrame."
      ],
      "metadata": {
        "id": "Kl4x4aiFj9YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the Census20_LSOA.shp layer into a GeoDataFrame.\n",
        "census20_LSOA = gpd.read_file('/content/drive/MyDrive/Colab Notebooks/london_data/Census20_LSOA.shp')\n",
        "\n",
        "# Make sure that the file loaded correctly\n",
        "census20_LSOA.head()"
      ],
      "metadata": {
        "id": "NfCn8qxTxQZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2 Subset the following columns (plus the geometry!):\n",
        "\n",
        "* LSOA11CD --> LSOA area code\n",
        "* LSOA11NM --> LSOA area name\n",
        "* LSOA11NMW --> LSOA bigger area\n",
        "* Pop20 --> Population counts"
      ],
      "metadata": {
        "id": "Ayw6j39NkotD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new GeoDataFrame with only the columns LSOA11CD, LSOA11NM, LSOA11NMW, Pop20, and geometry\n",
        "subset_census20_LSOA = census20_LSOA[[\"LSOA11CD\",\"LSOA11NM\",\"LSOA11NMW\",\"Pop20\", \"geometry\"]]\n",
        "\n",
        "#Make sure this worked properly\n",
        "subset_census20_LSOA.head()"
      ],
      "metadata": {
        "id": "cy84uavqygkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.3 What is the Coordinate Reference System of the layer?"
      ],
      "metadata": {
        "id": "5rqu6hRRmT26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the Coordinate Reference System of subset_census20_LSOA using .crs from the GeoPandas library\n",
        "print(subset_census20_LSOA.crs)"
      ],
      "metadata": {
        "id": "iUfBgCAbmY1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.4 How many features does the layer contain? Write an expression that returns the result as int."
      ],
      "metadata": {
        "id": "M-oNvkopnOaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the number of rows of subset_census20_LSOA as an integer using .shape[0]\n",
        "print(int(subset_census20_LSOA.shape[0]))"
      ],
      "metadata": {
        "id": "dfcT3tM4nbMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.5 The values in the \"LSOA11CD\" column (code area ID) should all be unique. How can you make sure?"
      ],
      "metadata": {
        "id": "q-aynok_tlHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether all values in the 'LSOA11CD' column are unique with .is_unique (returns True if all values are unique)\n",
        "print(subset_census20_LSOA['LSOA11CD'].is_unique)"
      ],
      "metadata": {
        "id": "h6wDrQ09zKib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.6 Plot the layer using the .plot method"
      ],
      "metadata": {
        "id": "_dsXb4x0uFc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot subset_census20_LSOA using .plot()\n",
        "subset_census20_LSOA.plot()\n"
      ],
      "metadata": {
        "id": "xCKdjxlI0jcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.7 Plot the layer using the .explorer method"
      ],
      "metadata": {
        "id": "4d7XO4BPuq3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot subset_census20_LSOA using .explore()\n",
        "subset_census20_LSOA.explore()"
      ],
      "metadata": {
        "id": "Db-SmMGZ0bEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.8 Subset just the LSOA areas with Pop20 counts greater than 1500"
      ],
      "metadata": {
        "id": "rtsoSNse0kFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Subset just the LSOA areas with Pop20 counts greater than 1500\n",
        "High_pop = subset_census20_LSOA[subset_census20_LSOA[\"Pop20\"] > 1500]\n",
        "\n",
        "#Make sure the new GeoDataFrame is correct\n",
        "High_pop.head()"
      ],
      "metadata": {
        "id": "ED5Ylka51Zah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.9 Plot the resulting subset, using symbology according to total population size, i.e., the \"Pop_Total\" column, and using a sequentual color map such as \"Reds\"."
      ],
      "metadata": {
        "id": "ZzB0DolO1w1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot High_pop according to total population size using .explore()\n",
        "High_pop.explore(column='Pop20', cmap='Reds', legend=True,  style_kwds={'weight': 0.5,'color': 'black'})\n",
        "# I chose to color the polygons by population using the sequential color map 'Reds.'\n",
        "# I added black borders to make the boundaries more visible"
      ],
      "metadata": {
        "id": "3kWr4gsA245e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.10 How many areas are there with the requested population?"
      ],
      "metadata": {
        "id": "U7aAXc6-4LNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the number of rows in High_Pop using .shape[0]\n",
        "print(High_pop.shape[0])"
      ],
      "metadata": {
        "id": "v6_IJ_tJ28PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.11 What is the total population of the subset layer?\n"
      ],
      "metadata": {
        "id": "e60pOQjQN5G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the sum of all of the values in the column 'Pop20' in High_pop using .sum()\n",
        "print(High_pop['Pop20'].sum())"
      ],
      "metadata": {
        "id": "G3C_uNDJ4lEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ec6fda"
      },
      "source": [
        "### 3.3 Practicing Rasterio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the EarthPy library\n",
        "!pip install earthpy"
      ],
      "metadata": {
        "id": "DA14gvRdCMcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import EarthPy and its spatial and plotting modules\n",
        "import earthpy as et\n",
        "import earthpy.spatial as es\n",
        "import earthpy.plot as ep"
      ],
      "metadata": {
        "id": "OLLccXIMCN6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.1 Read the file as a rasterio dataset"
      ],
      "metadata": {
        "id": "QqH-Kh8R9FVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define raster by opening the raster image <https://storage.googleapis.com/gcp-public-data-landsat/LC08/01/046/027/LC08_L1TP_046027_20181224_20190129_01_T1/LC08_L1TP_046027_20181224_20190129_01_T1_B8.TIF> using rasterio\n",
        "raster = rio.open('/content/drive/MyDrive/Colab Notebooks/Copy of Clipped_Raster.tif')"
      ],
      "metadata": {
        "id": "2GU0Rgyy4v45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.2 What is the CRS of the dataset?"
      ],
      "metadata": {
        "id": "UU6Z6Emq-Ayg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the CRS of the dataset using .crs\n",
        "print(raster.crs)"
      ],
      "metadata": {
        "id": "scA8mSKV-FtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.3 What is the raster extent (bounds) of the dataset in projected coordinates?"
      ],
      "metadata": {
        "id": "HeeiGJFe-NcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the extent of raster using .bounds\n",
        "print(raster.bounds)\n"
      ],
      "metadata": {
        "id": "7zN-HU5O6JzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.4 How many bands are there in this dataset?"
      ],
      "metadata": {
        "id": "Aj8E4bt5_KbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of bands in raster using .count\n",
        "print(raster.count)"
      ],
      "metadata": {
        "id": "u893jXBk_OIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.5 Create a plot of the image"
      ],
      "metadata": {
        "id": "pKzTgnKW_d3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a plot of the image\n",
        "# Define raster_arr (a NumPy array) as the first band of raster\n",
        "raster_arr = raster.read(1)\n",
        "\n",
        "# Display the array\n",
        "raster_arr"
      ],
      "metadata": {
        "id": "PkFoNTyt6g3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask out zero values to improve visualization\n",
        "image_read_masked = np.ma.masked_array(raster_arr, mask=(raster_arr == 0))\n",
        "\n",
        "# Set figure size for the plot\n",
        "plt.rcParams['figure.figsize'] = [10, 8]\n",
        "\n",
        "# Display the raster band using a grayscale colormap\n",
        "plt.imshow(image_read_masked, cmap=\"gist_gray\")"
      ],
      "metadata": {
        "id": "p7HfxOG8AJ09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.6 Create a histogram from the raster"
      ],
      "metadata": {
        "id": "3A77ZjmLEkGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a histogram of the raster values\n",
        "ep.hist(raster_arr, colors='Purple', title='Histogram')\n",
        "\n",
        "# Display histogram\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V23rQPNc6kpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.7 Using EarthPy create a plot of false color (if you have an issue whith this, please describe the potential cause)"
      ],
      "metadata": {
        "id": "kWtDXftVHrbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code I used, which created an error:\n",
        "```\n",
        "ep.plot_rgb(raster_arr,\n",
        "            rgb=[3, 2, 1],\n",
        "            stretch=True,\n",
        "            str_clip=10)\n",
        "plt.show()\n",
        "```\n"
      ],
      "metadata": {
        "id": "piGo3ZofJp7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There was an error because you cannot create a false-color plot with just one band. To fix this, I would need a raster file that contains at least three bands."
      ],
      "metadata": {
        "id": "ZMj9HyvHINGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Spatial Clustering (K-Means - DBSCAN)"
      ],
      "metadata": {
        "id": "W0i6FTqaKSd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Get the required data in your Drive"
      ],
      "metadata": {
        "id": "3DfRhA2YLZz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I put data in my drive."
      ],
      "metadata": {
        "id": "_ZUXj5JLWGvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Exploratory Data Analysis and K-means Clustering"
      ],
      "metadata": {
        "id": "8xZbJ18ZLgvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install additional libraries like Lonboard to display large datasets."
      ],
      "metadata": {
        "id": "UphIGTdkNHdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lonboard"
      ],
      "metadata": {
        "id": "BKh4lNijNJOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import shapely\n",
        "import folium\n",
        "import seaborn as sns\n",
        "from lonboard import Map, ScatterplotLayer, viz"
      ],
      "metadata": {
        "id": "WWCs93bxuPpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2 Part A: Data Exploration and Pre-Processing"
      ],
      "metadata": {
        "id": "OV_M8zC6LloP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2A.1 Use pandas to load the car accidents dataset"
      ],
      "metadata": {
        "id": "ZimTzbspubNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define accident_data as a DataFrame by reading UK_Accident.csv\n",
        "accident_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Road_Accidents_UK/UK_Accident.csv\")"
      ],
      "metadata": {
        "id": "3Tf5mHppumoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2A.2 Display the first few rows to understand the available attributes."
      ],
      "metadata": {
        "id": "xfklC_CMvFQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of accident_data by using .head()\n",
        "accident_data.head()"
      ],
      "metadata": {
        "id": "B4D3u28zvODT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2A.3 Keep only the necessary columns, have a mix of Numerical and Categorical attributes"
      ],
      "metadata": {
        "id": "DhbYRHPjvX_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at all the columns to choose the necessary attributes\n",
        "accident_data.columns"
      ],
      "metadata": {
        "id": "CGS9FiB5vaL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of the columns I want to keep\n",
        "keep_cols = [\"Accident_Index\", \"Longitude\", \"Latitude\", \"Date\", \"Day_of_Week\", \"Time\", \"Accident_Severity\", \"Number_of_Casualties\", \"Local_Authority_(District)\", \"Light_Conditions\", \"Road_Surface_Conditions\", \"Urban_or_Rural_Area\", \"LSOA_of_Accident_Location\", \"Year\",]\n",
        "# Subset accident_data so it only contains the columns in keep_cols\n",
        "accident_data = accident_data[keep_cols]\n",
        "# Display the first few rows to make sure the Dataframe was subsetted correctly\n",
        "accident_data.head()"
      ],
      "metadata": {
        "id": "l2uc2USevkTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2A.4 Slice (cut) the pandas dataframe by including only records from 2010, which will reduce your dataset to approx 770585 rows.\n"
      ],
      "metadata": {
        "id": "zzUGavwOwUgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset accident_data to include only records from the year 2010 onward\n",
        "accident_data_2010 = accident_data[accident_data['Year'] >= 2010]\n",
        "\n",
        "# Print the number of rows to make sure there are approx 770585\n",
        "print(accident_data_2010.shape[0])\n",
        "\n",
        "# Show the first few rows make sure the dates are 2010 onward\n",
        "accident_data_2010.head()"
      ],
      "metadata": {
        "id": "IBPgNUr4wYOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2A.5 Make a simple plot to represent which day of the week historically has had more car accidents. Which day?"
      ],
      "metadata": {
        "id": "YhcrPM4ew6CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count accidents per day of the week\n",
        "accidents_per_day = accident_data_2010[\"Day_of_Week\"].value_counts().sort_index()\n",
        "\n",
        "# Plot accident frequency by day of the week\n",
        "plt.figure(figsize=(10, 6))\n",
        "accidents_per_day.plot(kind='bar')\n",
        "\n",
        "plt.title(\"Number of Car Accidents by Day of the Week (2010+)\")\n",
        "plt.xlabel(\"Day of the Week\")\n",
        "plt.ylabel(\"Number of Accidents\")\n",
        "plt.show()\n",
        "\n",
        "# Print the day with the most accidents\n",
        "most_accidents_day = accidents_per_day.idxmax()\n",
        "print(\"Day with the most accidents: Day\", most_accidents_day)"
      ],
      "metadata": {
        "id": "0FGxLWqRy6qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2A.6 Make a second plot to explore the relationship between Accident Severity and Road Conditions. What insights can you gain about that?. Use a Text Cell reflecting on the previous charts."
      ],
      "metadata": {
        "id": "ICR7aE2pzQmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count accidents by Road Surface Conditions and Accident Severity\n",
        "severity_vs_road = accident_data_2010.groupby(['Road_Surface_Conditions', 'Accident_Severity']).size().unstack()\n",
        "\n",
        "# Plot as a stacked bar chart\n",
        "severity_vs_road.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='Reds')\n",
        "plt.xlabel(\"Road Surface Conditions\")\n",
        "plt.ylabel(\"Number of Accidents\")\n",
        "plt.title(\"Accident Severity vs Road Surface Conditions (2010+)\")\n",
        "plt.legend(title=\"Accident Severity\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B9hAZyo0zbLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram shows that most accidents occur on dry roads, followed by wet/damp and frost/ice conditions. Accidents of all severities generally follow this same pattern. While the absolute number of severe (3) and less severe (2) accidents is higher on dry roads than on wet roads, the proportion of severe accidents is actually higher on wet roads than on dry roads."
      ],
      "metadata": {
        "id": "5ueh8hSk0ZxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2A.7 Using Lonboard Library map all the car accidents included in the filtered dataset"
      ],
      "metadata": {
        "id": "XKfiI23x1cdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the GeoDataFrame\n",
        "geometry = gpd.points_from_xy(accident_data_2010.Longitude, accident_data_2010.Latitude)\n",
        "accident_gdf = gpd.GeoDataFrame(accident_data_2010, geometry=geometry, crs=\"EPSG:4326\")\n",
        "\n",
        "#Make sure it is constructed correctly\n",
        "accident_gdf.head()"
      ],
      "metadata": {
        "id": "L1Nua2sN5kmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz(accident_gdf)"
      ],
      "metadata": {
        "id": "FBKh6l0R2AmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2A.8 Make a spatial filter (create a new dataset) to map only the car accidents In the Glasgow-Edinburgh Region, create another map using the lonboard library to display the car accidents only in that region."
      ],
      "metadata": {
        "id": "o78u2mDj6jGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define bounding box for Glasgow-Edinburgh region: (min_lon, min_lat, max_lon, max_lat)\n",
        "ge_bbox = [    -4.721,55.735,-2.785,56.02]\n",
        "\n",
        "# Slice accident_gdf based on new boundaries and explicitly create a copy\n",
        "glasgow_edinburgh_accidents = accident_gdf[accident_gdf.intersects(shapely.box(*ge_bbox))].copy()\n",
        "\n",
        "# Check new dataset\n",
        "glasgow_edinburgh_accidents.head()"
      ],
      "metadata": {
        "id": "-PNMuYEc6itr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the filtered accidents using viz()\n",
        "viz(glasgow_edinburgh_accidents)"
      ],
      "metadata": {
        "id": "AFyYtWtw7vAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2 Part B K-means Clustering Implementation"
      ],
      "metadata": {
        "id": "73m9PMFoL78p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2B.1 Implement K-means clustering with different values of k. (e.g 3 and 5 clusters) To the filtered dataset you have created for the Glasgow-Edinburgh region."
      ],
      "metadata": {
        "id": "XNa01GKO8U9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize KMeans with 3 clusters and a random seed for reproducibility\n",
        "kmeans3 = KMeans(n_clusters=3, random_state=42)\n",
        "\n",
        "# Assign each data point to the cluster of the nearest centroid\n",
        "glasgow_edinburgh_accidents['kmeans_3'] = kmeans3.fit_predict(\n",
        "    glasgow_edinburgh_accidents[['Longitude', 'Latitude']]\n",
        ")\n",
        "\n",
        "# Initialize KMeans with 5 clusters and a random seed for reproducibility\n",
        "kmeans5 = KMeans(n_clusters=5, random_state=42)\n",
        "\n",
        "# Assign each data point to the cluster of the nearest centroid\n",
        "glasgow_edinburgh_accidents['kmeans_5'] = kmeans5.fit_predict(\n",
        "    glasgow_edinburgh_accidents[['Longitude', 'Latitude']]\n",
        ")\n",
        "\n",
        "#Verify the columns kmeans_5 and kmeans_3 were created\n",
        "glasgow_edinburgh_accidents.head()"
      ],
      "metadata": {
        "id": "Wwdm-y0bGTo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2B.2 Map the clusters using the lonboard library"
      ],
      "metadata": {
        "id": "rCm7acEbNVBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "#Map according to the 3 clusters\n",
        "fig_kmeans3 = px.scatter_mapbox(\n",
        "    glasgow_edinburgh_accidents,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"kmeans_3\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,\n",
        "    title=\"K-means Clustering (k=3) of Accidents in Edinburgh-Glasgow (2010+)\",\n",
        "    height=700,\n",
        "    opacity=0.5,\n",
        "\n",
        ")\n",
        "\n",
        "fig_kmeans3.show()\n",
        "\n",
        "#Map according to the 5 clusters\n",
        "fig_kmeans5 = px.scatter_mapbox(\n",
        "    glasgow_edinburgh_accidents,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"kmeans_5\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,\n",
        "    title=\"K-means Clustering (k=5) of Accidents in Edinburgh-Glasgow (2010+)\",\n",
        "    height=700,\n",
        "    opacity=0.5,\n",
        "\n",
        ")\n",
        "\n",
        "fig_kmeans5.show()"
      ],
      "metadata": {
        "id": "RR6CQY7mQseP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2B.3 Describe in a Text Cell the clustering results. How does the choice of k impact the clusters?. Describe how the clusters change once you adjust multiple versions of that required parameter."
      ],
      "metadata": {
        "id": "yP6aF52uTbZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clustered results reveal how accident clusters consistently appear around urban centers (Glasgow and Edinburgh).The smaller k (3) gives a general overview of hotspots but can obscure local patterns. The larger k (5) provides more detail but, in this case, may over-segment the data, making it harder to interpret the broader trend."
      ],
      "metadata": {
        "id": "oDbRsarpdWEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2B.4 Finally: In the guideline, we worked using only the coordinates to create the clusters (['Longitude', 'Latitude']), in another code cell, implement K-means clustering again, but now using the attributes included in the dataframe like Accident_Severity, Number_of_Vehicles."
      ],
      "metadata": {
        "id": "eY0unRlNUYqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select attributes to use for clustering\n",
        "features = glasgow_edinburgh_accidents[['Accident_Severity', 'Number_of_Casualties']]\n",
        "\n",
        "# Standardize the features so that each has mean 0 and variance 1\n",
        "# This avoids attributes with larger ranges dominating the clustering\n",
        "#scaler = StandardScaler()\n",
        "#features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Initialize KMeans with 3 clusters for example\n",
        "kmeans_attr = KMeans(n_clusters=3, random_state=42)\n",
        "\n",
        "# Fit KMeans and assign clusters based on accident attributes\n",
        "glasgow_edinburgh_accidents['kmeans_attr'] = kmeans_attr.fit_predict(features)\n",
        "\n",
        "# Check results\n",
        "glasgow_edinburgh_accidents.head()"
      ],
      "metadata": {
        "id": "YyMywprWfK5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2B.5 Visualise the results using the lonboard library."
      ],
      "metadata": {
        "id": "FRXjeKsBUdYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize clusters based on accident attributes on the map\n",
        "fig_attr_clusters = px.scatter_mapbox(\n",
        "    glasgow_edinburgh_accidents,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"kmeans_attr\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,\n",
        "    title=\"K-means Clustering (k=3) for Attributes: Severity & Casualties in Glasgow-Edinburgh (2010+)\",\n",
        "    height=700,\n",
        "    opacity=0.5,\n",
        ")\n",
        "\n",
        "# Show map\n",
        "fig_attr_clusters.show()"
      ],
      "metadata": {
        "id": "2YbS5-TafPJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2B.6 In a Text Cell, reflect on the clusters that include only the coordinates and the ones that also include other attributes. What insights can you gain about that?"
      ],
      "metadata": {
        "id": "PcHvGiKfUiNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When clustering using only the coordinates (Longitude and Latitude), the K-means algorithm groups accidents purely based on their spatial proximity. As a result, clusters tend to form around dense urban areas. When I included Accident_Severity and Number_of_Casualties, the clusters are influenced, not only by location, but also by the characteristics of the accidents. This allows me to identify areas where more severe accidents are more likely, even if those areas are not the densest in terms of accident counts."
      ],
      "metadata": {
        "id": "v1JkL9quhu45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Spatial Analysis and DBSCAN Clustering"
      ],
      "metadata": {
        "id": "Td6klpiKMN0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3 Part A: Spatial Correlation"
      ],
      "metadata": {
        "id": "YHHMv4aeMkYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.1 Create another GeoPandas Dataframe by rereading the data to avoid any confusion with the previous geodataframe. This new one is about DBSCAN name it accordingly."
      ],
      "metadata": {
        "id": "PO8k9UwoU3N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a new GeoDataFrame by making a copy of accident_data_2010 using .copy()\n",
        "accident_gdf_dbscan = accident_gdf.copy()"
      ],
      "metadata": {
        "id": "MOIPepE9Vwaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.2 Using the BBox website, filter the new geodataframe to contain only the accidents around Birmingham."
      ],
      "metadata": {
        "id": "JmfY0yoJU5W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define bounding box for Birmingham region: (min_lon, min_lat, max_lon, max_lat)\n",
        "birm_bbox = [-2.7,52.16,-1.12,52.8]\n",
        "\n",
        "# Slice accident_gdf based on new boundaries and explicitly create a copy\n",
        "birm_accidents_dbscan = accident_gdf_dbscan[accident_gdf_dbscan.intersects(shapely.box(*birm_bbox))].copy()\n",
        "\n",
        "# Check new dataset\n",
        "birm_accidents_dbscan.head()\n"
      ],
      "metadata": {
        "id": "gMbbWzGXj6M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.3 Using the Lonboard library, map the filtered dataset in Birmingham."
      ],
      "metadata": {
        "id": "wjqaVOYVVAXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the filtered accidents using viz()\n",
        "viz(birm_accidents_dbscan)"
      ],
      "metadata": {
        "id": "8aJPkFNklcpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.4 In a code cell, investigate the data type of the attribute list, so you can identify which attributes are numerical and which are categorical. Tip: use .dtypes\n"
      ],
      "metadata": {
        "id": "XE1_ir0Cl2p_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the data types of all columns\n",
        "birm_accidents_dbscan.dtypes"
      ],
      "metadata": {
        "id": "Sz2iLqXAnOWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.5 In a code cell. Run the correlation between the numerical attributes by including in your code corr= your_dataframe.corr()"
      ],
      "metadata": {
        "id": "Cf_4aoqGl5rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only include numerical columns to avoid errors with non-numeric data\n",
        "numerical_cols = birm_accidents_dbscan.select_dtypes(include='number')\n",
        "corr_matrix = numerical_cols.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "corr_matrix"
      ],
      "metadata": {
        "id": "d1EYsuYynWJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.6 In a new Code Cell, adjust the following code to create a heatmap plot of your correlation values."
      ],
      "metadata": {
        "id": "QK0WGSgsvfAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    corr_matrix,  #I changed this from your_corr_variable to corr_matrix\n",
        "    annot=True,\n",
        "    cmap='coolwarm',\n",
        "    fmt='.2f',\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={'label': 'Correlation Coefficient'}\n",
        ")\n",
        "\n",
        "plt.title('Pearson -Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ppOfbwW3wsGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.7 Install the library **pysal** by running in a code cell: `pip install pysal `\n"
      ],
      "metadata": {
        "id": "Gj5MCet0wK0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pysal"
      ],
      "metadata": {
        "id": "w9604KjaxcUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.8 Now import the new and requieres libraries."
      ],
      "metadata": {
        "id": "XFsIkGfrwR5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import libpysal.weights as weights\n",
        "from esda.moran import Moran"
      ],
      "metadata": {
        "id": "FJ4Lr125x6SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.9 You must reproject your dataset (recall the EPSG code you used in the guideline notebook to study spatial data in the UK)\n"
      ],
      "metadata": {
        "id": "BkI5mbr5wYSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the current CRS\n",
        "print(birm_accidents_dbscan.crs)\n"
      ],
      "metadata": {
        "id": "iY42NTpXGemM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproject the Birmingham accidents GeoDataFrame to EPSG:27700\n",
        "birm_accidents_dbscan_27700 = birm_accidents_dbscan.to_crs(epsg=27700)\n",
        "\n",
        "# Verify conversion\n",
        "birm_accidents_dbscan_27700.crs"
      ],
      "metadata": {
        "id": "4QqRqj0VySUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = weights.DistanceBand.from_dataframe(birm_accidents_dbscan_27700, threshold=500, ids=birm_accidents_dbscan_27700.index, silence_warnings=True)\n",
        "w.transform = 'R'\n",
        "moran = Moran(birm_accidents_dbscan_27700['Accident_Severity'], w)\n",
        "\n",
        "print(f\"\\n--- Moran's I Spatial Autocorrelation Analysis ---\")\n",
        "print(f\"Defined {w.n} observations and {w.mean_neighbors:.2f} average neighbors per point.\")\n",
        "print(f\"\\nMoran's I Statistic (Observed I): {moran.I:.4f}\")\n",
        "print(f\"P-value (significance): {moran.p_sim:.4f}\")"
      ],
      "metadata": {
        "id": "vexKjoFy7Au1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.10 In a Text Cell, describe with your own words the results., What insights can you gain from the correlation analysis."
      ],
      "metadata": {
        "id": "-xEt2Ym_ORKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Average neighbors = 40.49:** This means that each accident location has about 40 nearby points within 500 meters. This gives context to the calculation for Moran's I.\n",
        "\n",
        "**Moran's I Statistic= 0.0208:** This value is positive but very close to zero. This means that there is a very weak positive spatial correlation. In other words, locations with high accident severity are very slightly more likely to be near other locations with high severity.\n",
        "\n",
        "**P-value = 0.0010:** This is statistically significant. This means that even though the correlation is weak, it is unlikely due to random chance.\n",
        "\n"
      ],
      "metadata": {
        "id": "UyOGkbX2Pzin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3 Part B: DBSCAN Clustering Implementation"
      ],
      "metadata": {
        "id": "rFWsIiZ8Mlp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.1 Implement DBSCAN clustering with different eps and min_samples to the projected dataset."
      ],
      "metadata": {
        "id": "EjPZoVvjLx64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract X and Y coordinates to use in DBSCAN\n",
        "birm_accidents_dbscan_27700['geometry_x']=birm_accidents_dbscan_27700.geometry.x\n",
        "birm_accidents_dbscan_27700['geometry_y']=birm_accidents_dbscan_27700.geometry.y"
      ],
      "metadata": {
        "id": "ZGTc-O6T3rpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define DBSCAN parameters\n",
        "\n",
        "# Example 1 (small radius, medium min)\n",
        "eps_radius_1 = 50\n",
        "min_samples_1 = 40\n",
        "\n",
        "#Example 2 (medium radius, medium min)\n",
        "eps_radius_2 = 200\n",
        "min_samples_2 = 40\n",
        "\n",
        "# Example 3 (large radius, medium min)\n",
        "eps_radius_3 = 300\n",
        "min_samples_3 = 40\n",
        "\n",
        "# Example 4 (medium radius, small min)\n",
        "eps_radius_4 = 200\n",
        "min_samples_4 = 10\n",
        "\n",
        "#Example 5 (medium radius, medium min)\n",
        "eps_radius_5 = 200\n",
        "min_samples_5 = 40\n",
        "\n",
        "# Example 6 (medium radius, large min)\n",
        "eps_radius_6 = 300\n",
        "min_samples_6 = 70"
      ],
      "metadata": {
        "id": "BqjMXnyVIX1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run DBSCAN with different parameters\n",
        "\n",
        "# Initialize DBSCAN clustering using parameters above\n",
        "\n",
        "#Example 1\n",
        "dbscan_1 = DBSCAN(\n",
        "    eps=eps_radius_1,\n",
        "    min_samples=min_samples_1\n",
        ")\n",
        "birm_accidents_dbscan_27700['dbscan_labels_radius50m'] = dbscan_1.fit_predict(\n",
        "    birm_accidents_dbscan_27700[['geometry_x', 'geometry_y']]\n",
        ")\n",
        "\n",
        "#Example 2\n",
        "dbscan_2 = DBSCAN(\n",
        "    eps=eps_radius_2,\n",
        "    min_samples=min_samples_2\n",
        ")\n",
        "\n",
        "birm_accidents_dbscan_27700['dbscan_labels_radius200m'] = dbscan_2.fit_predict(\n",
        "    birm_accidents_dbscan_27700[['geometry_x', 'geometry_y']]\n",
        ")\n",
        "#Example 3\n",
        "dbscan_3 = DBSCAN(\n",
        "    eps=eps_radius_3,\n",
        "    min_samples=min_samples_3\n",
        ")\n",
        "birm_accidents_dbscan_27700['dbscan_labels_radius300m'] = dbscan_3.fit_predict(\n",
        "    birm_accidents_dbscan_27700[['geometry_x', 'geometry_y']]\n",
        ")\n",
        "\n",
        "#Example 4\n",
        "dbscan_4 = DBSCAN(\n",
        "    eps=eps_radius_4,\n",
        "    min_samples=min_samples_4\n",
        ")\n",
        "birm_accidents_dbscan_27700['dbscan_labels_min10'] = dbscan_4.fit_predict(\n",
        "    birm_accidents_dbscan_27700[['geometry_x', 'geometry_y']]\n",
        ")\n",
        "\n",
        "#Example 5\n",
        "dbscan_5 = DBSCAN(\n",
        "    eps=eps_radius_5,\n",
        "    min_samples=min_samples_5\n",
        ")\n",
        "birm_accidents_dbscan_27700['dbscan_labels_min40'] = dbscan_5.fit_predict(\n",
        "    birm_accidents_dbscan_27700[['geometry_x', 'geometry_y']]\n",
        ")\n",
        "\n",
        "#Example 6\n",
        "dbscan_6 = DBSCAN(\n",
        "    eps=eps_radius_6,\n",
        "    min_samples=min_samples_6\n",
        ")\n",
        "birm_accidents_dbscan_27700['dbscan_labels_min70'] = dbscan_6.fit_predict(\n",
        "    birm_accidents_dbscan_27700[['geometry_x', 'geometry_y']]\n",
        ")\n",
        "\n",
        "#Verify new columns\n",
        "birm_accidents_dbscan_27700.head()"
      ],
      "metadata": {
        "id": "0pB9_crY3pUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.2 Map the clusters using the Plotly Library"
      ],
      "metadata": {
        "id": "dDtA17eVVOD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "xs-o0aqdCDo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the maps using Plotly Express\n",
        "\n",
        "#Map 1 — DBSCAN with small radius and medium min\n",
        "fig_dbscan_radius50m = px.scatter_mapbox(\n",
        "    birm_accidents_dbscan_27700,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"dbscan_labels_radius50m\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,\n",
        "    title=\"DBSCAN Clustering (eps=50m, min_samples=40) in Birmingham (2010+)\",\n",
        "    height=700,\n",
        "    opacity=0.5\n",
        ")\n",
        "\n",
        "#Map 2 — DBSCAN with medium radius and medium min\n",
        "fig_dbscan_radius200m = px.scatter_mapbox(\n",
        "    birm_accidents_dbscan_27700,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"dbscan_labels_radius200m\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,\n",
        "    title=\"DBSCAN Clustering (eps=200m, min_samples=40) in Birmingham (2010+)\",\n",
        "    height=700,\n",
        "    opacity=0.5\n",
        ")\n",
        "\n",
        "#Map 3 — DBSCAN with large radius and medium min\n",
        "fig_dbscan_radius300m = px.scatter_mapbox(\n",
        "    birm_accidents_dbscan_27700,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"dbscan_labels_radius300m\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,\n",
        "    title=\"DBSCAN Clustering (eps=300m, min_samples=40) in Birmingham (2010+)\",\n",
        "    height=700,\n",
        "    opacity=0.5\n",
        ")\n",
        "\n",
        "#Map 4 — DBSCAN with medium radius and small min\n",
        "fig_dbscan_min10 = px.scatter_mapbox(\n",
        "    birm_accidents_dbscan_27700,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"dbscan_labels_min10\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,\n",
        "    title=\"DBSCAN Clustering (eps=200m, min_samples=10) in Birmingham (2010+)\",\n",
        "    height=700,\n",
        "    opacity=0.5\n",
        ")\n",
        "\n",
        "#Map 5 — DBSCAN with medium radius and medium min\n",
        "fig_dbscan_min40 = px.scatter_mapbox(\n",
        "    birm_accidents_dbscan_27700,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"dbscan_labels_min40\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,\n",
        "    title=\"DBSCAN Clustering (eps=200m, min_samples=40) in Birmingham (2010+)\",\n",
        "    height=700,\n",
        "    opacity=0.5\n",
        ")\n",
        "\n",
        "#Map 3 — DBSCAN with medium radius and large min\n",
        "fig_dbscan_min70 = px.scatter_mapbox(\n",
        "    birm_accidents_dbscan_27700,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"dbscan_labels_min70\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,\n",
        "    title=\"DBSCAN Clustering (eps=200m, min_samples=70) in Birmingham (2010+)\",\n",
        "    height=700,\n",
        "    opacity=0.5\n",
        ")"
      ],
      "metadata": {
        "id": "62PVu1VTCVJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show maps\n",
        "\n",
        "# Map 1\n",
        "fig_dbscan_radius50m.show()\n",
        "# Map 2\n",
        "fig_dbscan_radius200m.show()\n",
        "# Map 3\n",
        "fig_dbscan_radius300m.show()\n",
        "# Map 3\n",
        "fig_dbscan_min10.show()\n",
        "# Map 3\n",
        "fig_dbscan_min40.show()\n",
        "# Map 3\n",
        "fig_dbscan_min70.show()"
      ],
      "metadata": {
        "id": "0ekwNU9o6D8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.3 Describe in a Text Cell the clustering results. How does the choice of eps and min_samples impact the clusters?. Describe how the clusters change once you adjust multiple versions of that required parameter."
      ],
      "metadata": {
        "id": "3LZcMwwtVSlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the eps controls the physical scale of clusters, the min_samples control how dense a location must be to count as a hotspot. In my maps, I noticed how the plots with smaller eps show almost no hotspots, and large eps show clusters that merge into broad regional hotspots. In terms of the min_samples, Map 4 (with a low minimum of 10) show many small, local clusters while Map 6 (with a high minimum of 70) only highlights the strongest and densest hotspots. Overall, these maps illustrate perfectly how DBSCAn can be tuned to detect either neighborhood-level accident risk or only the small extreme clusters."
      ],
      "metadata": {
        "id": "ttuaGkXSAUmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.4  In a Text Cell, briefly reflect on the clusters created using K-Means and the ones generated with DBSCAN. What insights can you gain from that?, Do you see any limitations?"
      ],
      "metadata": {
        "id": "HgxzY19zVWog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means produces a fixed number of clusters, forcing all accidents into groups even where no true hotspot exists. This can distort real accident concentrations, especially along long roads or irregular intersections. This is a real limitation of K-means. For this reason, K-means is best for identifying centroid-based groupings. In contrast, DBSCAN identifies clusters based on actual density patterns and can find irregularly shaped hotspots that follow road networks. I found that DBSCAN's main limitation is its sensitivity to parameter choices. Small changes in eps and min_samples can dramatically change the output and obscure important clusters and hotspots."
      ],
      "metadata": {
        "id": "8wp-cKBLAVNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.5 Finally, in a new text cell address the following question: What do you think are the real-world implications of the identified clusters in the field of urban planning?\n"
      ],
      "metadata": {
        "id": "WAafQBZ2VbGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These accident clusters have strong real-world implications for urban planning. Identifying hotspots helps planners prioritise locations for road redesign, new signage, speed-calming measures, improved lighting, or pedestrian crossings. Furthermore, cluster analysis also shows where resources should be allocated by identifying certain disadvantaged regions. Overall, these spatial patterns help planners make data-driven decisions to reduce future accidents and improve equity."
      ],
      "metadata": {
        "id": "-s029_c2AWHA"
      }
    }
  ]
}